#### pipeline for detecting viral integrations in NGS data ####

# the pipeline detecets viral integrations into a host genome in short reads by 
# identifying chimeric reads, discordant read pairs, and short insertions of virus 
# flanked by host sequence on both sides

# note that only paired-end reads are supported

# the pipeline relies on a particular directory structure:
 
# parent_folder/
# ├── data
# │   ├── metadata
# │   │   └── dataset1
# │   ├── reads
# │   │   └── dataset1
# │   │       ├── sample1_1.fastq.gz
# │   │       ├── sample2_2.fastq.gz
# │   │       ├── sample2_1.fastq.gz
# │   │       └── sample2_2.fastq.gz
# │   └── references
# │       ├── host.fa
# │       └── virus.fa
# ├── intvi_pipeline 
# │   └── all pipeline files from git repo
# └── proj-spcfc
#     └── dsets.yaml
#
# the folder data contains all the data necessary to run the pipeline
# within the data folder, reads should be organised into folders representing
# datasets, with gzipped paired fastq files for each sample named as above
# references should be placed in data/references.  A yaml file in proj-spcfc
# specifies which references are to be used with which datasets
# the pipline lives in the intvi_pipeline folder which should be cloned
# from the git repo

# the steps in the pipeline are as follows:
# 1. preprocessing
#   - rename all files for consistency (files may either be named "<sample>_1.fastq.gz" or "<sample>_R1.fastq.gz")
#   - either merge R1 and R2 using seqprep, or don't do merging 
#     and just concatenate R1 and R2, as specified in the dsets.yaml config file
# 2. alignments
#   - align all the reads to the virus
#   - remove duplicates
#   - extract only the alinged reads to a fastq
#   - align these reads to the host
# 3. perl scripts
#   - run perl scripts with alignment sam  files as inputs to detect integrations
# 4. postprocessing
#   - apply various types of postprocessing: dedup, filter, mask, annotate
#   - generate ouput xlsx files for each dataset
#   - write output files for visualzation in UCSC browser
#  
# the primary output of the pipeline is the xlsx files for each datasets which
# contain the details of the detected integrations.


#### python modules ####

import glob
import os

#### CONFIG FILE ####
# supply on command line

references_path = "../data/references/"

#make four lists, with each entry in each list corresponding to one desired output file

DATASETS = []
SAMPLES = []
HOSTS = []
VIRUSES = []
#also make dictionary MERGE, with key dataset_sample and value True or False
MERGE = {}

# we need to know if input files are bam or fastq.  check for filename extensions
# currently only support fastq (.fq.gz, .fq, .fastq.gz, .fastq)
# and bam (.bam, .sam)

for dataset in config:
	# get files in directory and strip off suffix (specified in "R1_suffix" field)
	# for fastq files
	if "R1_suffix" in config[dataset]:
		suffix = config[dataset]["R1_suffix"]	
		samples = [os.path.basename(f)[:-len(suffix)] for f in glob.glob(f"../data/reads/{dataset}/*{suffix}")]
		if len(samples) == 0:
			print(f"warning: no files found for dataset {dataset}")
	# for bam/sam files
	else:
		samples = [os.path.basename(f)[:-4] for f in glob.glob(f"../data/reads/{dataset}/*.bam")]
		if len(samples) > 0:
			config[dataset]['bam_suffix'] = ".bam"
		else:
			samples = [os.path.basename(f)[:-4] for f in glob.glob(f"../data/reads/{dataset}/*.sam")]
			if len(samples) > 0:
				config[dataset]['bam_suffix'] = ".sam"
			else:
				print(f"warning: no files found for dataset {dataset}")
			
	for sample in samples:
		DATASETS.append(dataset)
		SAMPLES.append(sample)
		HOSTS.append(config[dataset]["host"])
		VIRUSES.append(config[dataset]["virus"])
		MERGE["{}_{}".format(dataset, sample)] = config[dataset]["merge"]


# construct arguments for postprocess.R script for each dataset
POSTARGS = {}
TOSORT = []
SORTED = []
for dataset in config:
	POSTARGS[dataset] = []
	if "post" in config[dataset]:
		for element in config[dataset]["post"]:
			# need to check if this element is a string or a dict
			if type(element) is str:
				# look for "filter"
				if element == "filter":
					POSTARGS[dataset].append("filter")
				elif element == "dedup":
					POSTARGS[dataset].append("dedup")
			# postprocessing types with files specified will be in ordered dictionaries
			elif type(element) is OrderedDict:
				if "mask-exclude" in element.keys():
					for bed in element["mask-exclude"]:
						POSTARGS[dataset].append("mask-exclude")
						POSTARGS[dataset].append(bed)	
				elif "mask-include" in element.keys():
					for bed in element["mask-include"]:
						POSTARGS[dataset].append("mask-include")
						POSTARGS[dataset].append(bed)
				elif "nearest-gtf" in element.keys():
					for gtf in element["nearest-gtf"]:
						sortedgtf = os.path.splitext(gtf)[0] + ".sorted.gtf"
						POSTARGS[dataset].append("nearest-gtf")
						POSTARGS[dataset].append(sortedgtf)
						if gtf not in TOSORT:
							TOSORT.append(gtf)
							SORTED.append(sortedgtf)
				elif "nearest-bed" in element.keys():
					for bed in element["nearest-bed"]:
						sortedbed = os.path.splitext(bed)[0] + ".sorted.bed"
						POSTARGS[dataset].append("nearest-bed")
						POSTARGS[dataset].append(sortedbed)
						if bed not in TOSORT:
							TOSORT.append(bed)
							SORTED.append(sortedbed)
				elif "RNA-seq" in element.keys():
					ref = element["genes"]
					sortedref = os.path.splitext(ref)[0] + ".sorted" + os.path.splitext(ref)[1]
					if ref not in TOSORT:
						TOSORT.append(ref)
						SORTED.append(sortedref)
					for tsv in element["counts"]:
						POSTARGS[dataset].append("RNA-seq-gtf")
						POSTARGS[dataset].append(sortedref)
						POSTARGS[dataset].append(element["col"])
						POSTARGS[dataset].append(tsv)


#### local rules ####
localrules: all, combine, ucsc_bed, post

#### target files ####
rule all:
	input: 
		expand("../out/{dset}/host_aligned/{samp}.{host}.bwa.{virus}Mappedreads.bam", zip, dset = DATASETS, samp=SAMPLES, host=HOSTS, virus=VIRUSES),
		expand("../out/{dset}/host_aligned/{samp}.{host}.bwaPaired.{virus}Mappedreads.bam", zip, dset = DATASETS, samp=SAMPLES, host=HOSTS, virus=VIRUSES),
		expand("../out/{dset}/host_aligned/{samp}.{host}.bwaShort.{virus}Mappedreads.bam", zip, dset = DATASETS, samp=SAMPLES, host=HOSTS, virus=VIRUSES),
		expand("../out/{dset}/virus_aligned/{samp}.{virus}.bwa.bam", zip, dset = DATASETS, samp=SAMPLES, virus=VIRUSES),		
		expand("../out/{dset}/virus_aligned/{samp}.{virus}.bwaPaired.bam", zip, dset = DATASETS, samp=SAMPLES, virus=VIRUSES),
		expand("../out/{dset}/virus_aligned/{samp}.{virus}.bwaShort.bam", zip, dset = DATASETS, samp=SAMPLES, virus=VIRUSES),
		"../out/summary/num_sites.xlsx",
		expand("../out/summary/ucsc_bed/{dset}.post.bed", dset = DATASETS)

#### get reads from bam files if necesssary ####

rule bamtobed:
	input: lambda wildcards: f"../data/reads/{wildcards.dset}/{wildcards.samp}{config[wildcards.dset]['bam_suffix']}"
	output: 
		r1 = temp("../out/reads/{dset}/{samp}_1.fastq.gz"),
		r2 = temp("../out/reads/{dset}/{samp}_2.fastq.gz")
	params:
		sortedBam = lambda wildcards: f"../out/reads/{wildcards.dset}/{wildcards.samp}.sorted{config[wildcards.dset]['bam_suffix']}",
		unzip_r1 = lambda wildcards: f"../out/reads/{wildcards.dset}/{wildcards.samp}_1.fastq",
		unzip_r2 = lambda wildcards: f"../out/reads/{wildcards.dset}/{wildcards.samp}_2.fastq"
	conda:
		"envs/bwa.yml"
	shell:
		"""
		samtools sort -n -o {params.sortedBam} {input}
		bedtools bamtofastq -i {params.sortedBam} -fq {params.unzip_r1} -fq2 {params.unzip_r2}
		rm {params.sortedBam}
		gzip {params.unzip_r1}
		gzip {params.unzip_r2}
		"""
		
#### merging and dedup ###
def dedup_r1(wildcards):
	if "R1_suffix" in config[dataset]:
		return f"../data/reads/{wildcards.dset}/{wildcards.samp}{config[wildcards.dset]['R1_suffix']}"
	else:
		return f"../out/reads/{wildcards.dset}/{wildcards.samp}_1.fastq.gz"

def dedup_r2(wildcards):
	if "R2_suffix" in config[dataset]:
		return f"../data/reads/{wildcards.dset}/{wildcards.samp}{config[wildcards.dset]['R2_suffix']}"
	else:
		return f"../out/reads/{wildcards.dset}/{wildcards.samp}_2.fastq.gz"


rule dedup:
# remove exact duplicates (subs=0)
	input:
		r1 = dedup_r1,
		r2 = dedup_r2
	output:
		r1 = temp("../out/reads/{dset}/{samp}_1.dedup.fastq.gz"),
		r2 = temp("../out/reads/{dset}/{samp}_2.dedup.fastq.gz")
	conda:
		"envs/bbmap.yml"
	shell:
		"""
		clumpify.sh -Xmx15g in1="{input.r1}" in2="{input.r2}" out1="{output.r1}" out2="{output.r2}" dedupe subs=0
		"""

# input functions for if we want to do deduplication or not
def get_r1_for_seqprep(wildcards):
	if config[wildcards.dset]['dedup'] == "True":
		return f"../out/reads/{wildcards.dset}/{wildcards.samp}_1.dedup.fastq.gz"
	else:
		return f"../data/reads/{wildcards.dset}/{wildcards.samp}{config[wildcards.dset]['R1_suffix']}"


def get_r2_for_seqprep(wildcards):
	if config[wildcards.dset]['dedup'] == "True":
		return f"../out/reads/{wildcards.dset}/{wildcards.samp}_2.dedup.fastq.gz"
	else:
		return f"../data/reads/{wildcards.dset}/{wildcards.samp}{config[wildcards.dset]['R2_suffix']}"


rule seqPrep:
# if we're doing it
	input:
		r1 = get_r1_for_seqprep,
		r2 = get_r2_for_seqprep
	output:
		merged = temp("../out/{dset}/merged_reads/{samp}.SeqPrep_merged.fastq.gz"),
		proc_r1 = temp("../out/{dset}/merged_reads/{samp}.SeqPrep_1.fastq.gz"),
		proc_r2 = temp("../out/{dset}/merged_reads/{samp}.SeqPrep_2.fastq.gz"),
		all = temp("../out/{dset}/merged_reads/{samp}.all.fastq.gz")
	conda:	
		"envs/seqprep.yml"
	params:
		A = lambda wildcards: config[wildcards.dset]["read1-adapt"],
		B = lambda wildcards: config[wildcards.dset]["read2-adapt"]
	shell:
		"""
		SeqPrep -A {params.A} -B {params.B} -f {input.r1} -r {input.r2} -1 {output.proc_r1} -2 {output.proc_r2} -s {output.merged}
		cat {output.proc_r1} {output.proc_r2} {output.merged} > {output.all}
		"""

rule combine:
# for if we don't want to do seqPrep
	input:
		r1 = get_r1_for_seqprep,
		r2 = get_r2_for_seqprep
	output:
		proc_r1 = temp("../out/{dset}/processed_reads/{samp}.1.fastq.gz"),
		proc_r2 = temp("../out/{dset}/processed_reads/{samp}.2.fastq.gz"),
		all = temp("../out/{dset}/processed_reads/{samp}.all.fastq.gz")
	shell:
		"""
		cp {input.r1} {output.proc_r1}
		cp {input.r2} {output.proc_r2}
		cat {input.r1} {input.r2} > {output.all}
		"""
#functions for if we did seqPrep or not
def get_all_for_align(wildcards):
	if MERGE["{}_{}".format(wildcards.dset, wildcards.samp)] == "True":
		folder = "merged_reads"
	else:
		folder = "processed_reads"
	return "../out/{}/{}/{}.all.fastq.gz".format(wildcards.dset, folder, wildcards.samp)

def get_r1_for_align(wildcards):
	if MERGE["{}_{}".format(wildcards.dset, wildcards.samp)] == "True":
		typeRead = "SeqPrep_1"
		folder = "merged_reads"
	else:
		typeRead = "1"	
		folder = "processed_reads"
	return "../out/{}/{}/{}.{}.fastq.gz".format(wildcards.dset, folder, wildcards.samp, typeRead)

def get_r2_for_align(wildcards):
	if MERGE["{}_{}".format(wildcards.dset, wildcards.samp)] == "True":
		typeRead = "SeqPrep_2"
		folder = "merged_reads"
	else:
		typeRead = "2"	
		folder = "processed_reads"
	return "../out/{}/{}/{}.{}.fastq.gz".format(wildcards.dset, folder, wildcards.samp, typeRead)


#### alignments ####

rule index:
	input:
		"../data/references/{genome}.fa"
	output:
		"../data/references/{genome}.ann",
		"../data/references/{genome}.amb",
		"../data/references/{genome}.bwt",
		"../data/references/{genome}.pac",
		"../data/references/{genome}.sa"
	conda: 
		"envs/bwa.yml"
	params:
		prefix = lambda wildcards: references_path + wildcards.genome
	shell:
		"bwa index -p {params.prefix} {input}"

rule align_bwa_virus:
	input:
		ann = "../data/references/{virus}.ann",
		amb = "../data/references/{virus}.amb",
		bwt = "../data/references/{virus}.bwt",
		pac = "../data/references/{virus}.pac",
		sa = "../data/references/{virus}.sa",
		all = get_all_for_align,
		r1 = get_r1_for_align,
		r2 = get_r2_for_align
	output:
		vPaired = temp("../out/{dset}/virus_aligned/{samp}.{virus}.bwaPaired.sam"),
		vSing = temp("../out/{dset}/virus_aligned/{samp}.{virus}.bwa.sam"),
		vShort = temp("../out/{dset}/virus_aligned/{samp}.{virus}.bwaShort.sam")
	params:
		index = lambda wildcards: references_path + wildcards.virus
	conda: 
		"envs/bwa.yml"
	threads: 5
	shell:	
		"""
		python ./alignReadsWithBWA.py --threads {threads} --index {params.index} --read1 {input.r1} --read2 {input.r2} --output {output.vPaired} --threshold 10 --hflag 200
		python ./alignReadsWithBWA.py --threads {threads} --index {params.index} --read1 {input.all} --output {output.vSing} --threshold 10 --hflag 200
		python ./alignReadsWithBWA.py --threads {threads} --index {params.index} --read1 {input.all} --output {output.vShort} --threshold 10 --hflag 200  
		"""

rule extract_vAligned:
	input:
		vPaired = "../out/{dset}/virus_aligned/{samp}.{virus}.bwaPaired.sam",
		vSing = "../out/{dset}/virus_aligned/{samp}.{virus}.bwa.sam",
		vShort = "../out/{dset}/virus_aligned/{samp}.{virus}.bwaShort.sam"

	output:
		svSam = temp("../out/{dset}/virus_aligned/{samp}.{virus}.bwa.mapped.sam"),
		shortvSam = temp("../out/{dset}/virus_aligned/{samp}.{virus}.bwaShort.mapped.sam"),
		pvBam_readMap_mateUnmap = temp("../out/{dset}/virus_aligned/{samp}.{virus}.bwaPaired.mapped1.bam"),
		pvBam_readUnmap_mateMap = temp("../out/{dset}/virus_aligned/{samp}.{virus}.bwaPaired.mapped2.bam"),
		pvBam_bothMapped = temp("../out/{dset}/virus_aligned/{samp}.{virus}.bwaPaired.mapped3.bam"),
		pvBam = temp("../out/{dset}/virus_aligned/{samp}.{virus}.bwaPaired.mapped.bam"),
		pvSam = temp("../out/{dset}/virus_aligned/{samp}.{virus}.bwaPaired.mapped.sam")
	conda:
		"envs/bwa.yml"
	shell:
		"""
		samtools view -h -F 0x4 -F 0x800 -o {output.svSam} {input.vSing} 
		samtools view -h -F 0x4 -F 0x800 -o {output.shortvSam} {input.vShort} 
		samtools view -hb -F 0x4 -f 0x8 -F 0x800 -o {output.pvBam_readMap_mateUnmap} {input.vPaired}
		samtools view -hb -f 0x4 -F 0x8 -F 0x800 -o {output.pvBam_readUnmap_mateMap} {input.vPaired}
		samtools view -hb -F 0x4 -F 0x8 -F 0x800 -o {output.pvBam_bothMapped} {input.vPaired}
		samtools merge {output.pvBam} {output.pvBam_readMap_mateUnmap} {output.pvBam_bothMapped} {output.pvBam_readUnmap_mateMap}
		samtools view -h -o {output.pvSam} {output.pvBam}
		"""

rule extract_vAligedtoFastq:
	input: 
		svSam = "../out/{dset}/virus_aligned/{samp}.{virus}.bwa.mapped.sam",
		pvSam = "../out/{dset}/virus_aligned/{samp}.{virus}.bwaPaired.mapped.sam",
		shortvSam = "../out/{dset}/virus_aligned/{samp}.{virus}.bwaShort.mapped.sam" 
	output:
		svFastq = temp("../out/{dset}/reads/{samp}.bwa.mappedTo{virus}.fastq.gz"),
		shortvFastq = temp("../out/{dset}/reads/{samp}.bwaShort.mappedTo{virus}.fastq.gz"),
		pvFastq1 = temp("../out/{dset}/reads/{samp}.bwaPaired.mappedTo{virus}_1.fastq.gz"),
		pvFastq2 = temp("../out/{dset}/reads/{samp}.bwaPaired.mappedTo{virus}_2.fastq.gz")
	conda:
		"envs/picard.yml"
	shell:
		"""
		picard SamToFastq I={input.svSam} FASTQ={output.svFastq}
		picard SamToFastq I={input.shortvSam} FASTQ={output.shortvFastq}
		picard SamToFastq I={input.pvSam} FASTQ={output.pvFastq1} SECOND_END_FASTQ={output.pvFastq2}
		"""

rule align_bwa_host:
	input:	
		ann = "../data/references/{host}.ann",
		amb = "../data/references/{host}.amb",
		bwt = "../data/references/{host}.bwt",
		pac = "../data/references/{host}.pac",
		sa = "../data/references/{host}.sa",
		all = "../out/{dset}/reads/{samp}.bwa.mappedTo{virus}.fastq.gz",
		allShort = "../out/{dset}/reads/{samp}.bwaShort.mappedTo{virus}.fastq.gz",
		r1 = "../out/{dset}/reads/{samp}.bwaPaired.mappedTo{virus}_1.fastq.gz",
		r2 = "../out/{dset}/reads/{samp}.bwaPaired.mappedTo{virus}_2.fastq.gz"

	output:
		hPaired = temp("../out/{dset}/host_aligned/{samp}.{host}.bwaPaired.{virus}Mappedreads.sam"),
		hSing = temp("../out/{dset}/host_aligned/{samp}.{host}.bwa.{virus}Mappedreads.sam"),
		hShort = temp("../out/{dset}/host_aligned/{samp}.{host}.bwaShort.{virus}Mappedreads.sam")
	conda: 
		"envs/bwa.yml"
	params:
		index = lambda wildcards: references_path + wildcards.host
	threads: 5
	shell:		
		"""
		python ./alignReadsWithBWA.py --threads {threads} --index {params.index} --read1 {input.r1} --read2 {input.r2} --output {output.hPaired} --threshold 10 --hflag 200
		python ./alignReadsWithBWA.py --threads {threads} --index {params.index} --read1 {input.all} --output {output.hSing} --threshold 10 --hflag 200
		python ./alignReadsWithBWA.py --threads {threads} --index {params.index} --read1 {input.allShort} --output {output.hShort} --threshold 10 --hflag 200 --insert_extend 0
		"""
rule convert:
	input:
		"../out/{dset}/{host_virus}/{name}.sam"
	output:
		bam = "../out/{dset}/{host_virus}/{name}.bam",
		bai = "../out/{dset}/{host_virus}/{name}.bam.bai"
	conda: 
		"envs/bwa.yml"	
	shell:
		"""
		samtools view -bhS {input} | samtools sort - -o {output.bam}
		samtools index {output.bam}
		"""

#### perl scripts ####

rule run_int_scripts:
	input:
		hPaired = "../out/{dset}/host_aligned/{samp}.{host}.bwaPaired.{virus}Mappedreads.sam",
		hSing = "../out/{dset}/host_aligned/{samp}.{host}.bwa.{virus}Mappedreads.sam",
		hShort = "../out/{dset}/host_aligned/{samp}.{host}.bwaShort.{virus}Mappedreads.sam",
		vPaired = "../out/{dset}/virus_aligned/{samp}.{virus}.bwaPaired.sam",
		vSing = "../out/{dset}/virus_aligned/{samp}.{virus}.bwa.sam",
		vShort = "../out/{dset}/virus_aligned/{samp}.{virus}.bwaShort.sam",

	output:
		allInt = "../out/{dset}/ints/{samp}.{host}.{virus}.integrations.txt",
		short = temp("../out/{dset}/ints/{samp}.{host}.{virus}.short.txt"),
		soft = temp("../out/{dset}/ints/{samp}.{host}.{virus}.soft.txt"),
		discord = temp("../out/{dset}/ints/{samp}.{host}.{virus}.discordant.txt"),
		allIntTemp = temp("../out/{dset}/ints/{samp}.{host}.{virus}.integrations.txt.tmp"),

	shell:
		"""
		perl -I. ./softClip.pl --viral {input.vSing} --human {input.hSing} --output {output.soft} --tol 3
		perl -I. ./discordant.pl --viral {input.vPaired} --human {input.hPaired} --output {output.discord} --tol 3
		perl -I. ./short.pl --viral {input.vShort} --human {input.hShort} --output {output.short} --tol 3
		sed -e '2,${{/Chr/d' -e '}}' {output.soft} {output.discord} {output.short} > {output.allInt}
		sort -n -k1,1 -k2,2n {output.allInt} > {output.allIntTemp}
		cp {output.allIntTemp} {output.allInt}
		"""

	
#### postprocessing ####

rule count_mapped:
	input:
		expand("../out/{dset}/host_aligned_bam/{samp}.{host}.bwa.{virus}Mappedreads.bam", zip, dset = DATASETS, samp = SAMPLES, host = HOSTS, virus = VIRUSES),
		expand("../out/{dset}/virus_aligned_bam/{samp}.{virus}.bwa.bam", zip, dset = DATASETS, samp = SAMPLES, virus = VIRUSES),
		expand("../out/{dset}/host_aligned_bam/{samp}.{host}.bwaPaired.{virus}Mappedreads.bam", zip, dset = DATASETS, samp = SAMPLES, host = HOSTS, virus=VIRUSES),
		expand("../out/{dset}/virus_aligned_bam/{samp}.{virus}.bwaPaired.bam", zip, dset = DATASETS, samp = SAMPLES, virus = VIRUSES)
	output:
		"../out/summary/count_mapped.txt"
	conda: 
		"envs/bwa.yml"
	shell:
		"./count_mapped.sh"

rule sortbed:
	input:
		TOSORT
	output:
		SORTED
	run:
		for i in range(len(TOSORT)):
			print(f"sorting file {TOSORT[i]} into file {SORTED[i]}, file extension {os.path.splitext(TOSORT[i])[1]}")
			
			# if file is a bed file
			if (os.path.splitext(TOSORT[i])[1] == ".bed"):
				body = f"sort -k1,1 -k2,2n {TOSORT[i]} > {SORTED[i]}"
				shell(body)
				print(body)
			# if file is a gtf file
			elif (os.path.splitext(TOSORT[i])[1] == ".gtf"):
				
				body = f"awk '{{{{ if ($0 !~ /^#/) {{{{ print $0 }}}} }}}}' {TOSORT[i]} | sort -k1,1 -k4,4n > {SORTED[i]}"
				shell(body)
				print(body)
			else:
				raise ValueError("only gtf files and bed files are supported")


#this rule (post) sometimes causes issues with conda. The error is usually something to do with ldpaths:

#Activating conda environment: /scratch1/sco305/intvi_cmri/intvi_pipeline/.snakemake/conda/586e76e5
#/scratch1/sco305/intvi_cmri/intvi_pipeline/.snakemake/conda/586e76e5/lib/R/bin/R: line 238: /scratch1/sco305/intvi_cmri/#intvi_pipeline/.snakemake/conda/586e76e5/lib/R/etc/ldpaths: No such file or directory

# however, sometimes this rule runs just fine.

# this issue is descrived here: https://github.com/conda-forge/r-base-feedstock/issues/67
# however, it doesn't appear to have been resolved.  temporarily get around this by re-attempting jobs
# that failed using command line option --restart-times, but will need to come up with a better solution for this

rule post:
	input:
		"../out/{dset}/ints/{samp}.{host}.{virus}.integrations.txt",
		SORTED
	output:
		"../out/{dset}/ints/{samp}.{host}.{virus}.integrations.post.txt"
	conda:
		"envs/rscripts.yml"
	params:
		lambda wildcards: " ".join([f"../out/{wildcards.dset}/ints/{wildcards.samp}.{wildcards.host}.{wildcards.virus}.integrations.txt"] + POSTARGS[wildcards.dset])
	shell:
		"""
		Rscript post/postprocess.R {params}
		"""

	
rule summarise:
	input:
		expand("../out/{dset}/ints/{samp}.{host}.{virus}.integrations.post.txt", zip, dset = DATASETS, samp = SAMPLES, host = HOSTS, virus = VIRUSES)

	output:
		"../out/summary/num_sites.xlsx"
	conda:
		"envs/rscripts.yml"
	shell:
		"Rscript summarise_ints.R {input}"

rule ucsc_bed:
	input:
		expand("../out/{dset}/ints/{samp}.{host}.{virus}.integrations.post.txt", zip, dset = DATASETS, samp = SAMPLES, host = HOSTS, virus = VIRUSES)
	output:
		expand("../out/summary/ucsc_bed/{dset}.post.bed", dset = set(DATASETS)),
	conda:
		"envs/rscripts.yml"
	shell:
		"""
		Rscript writeBed.R {input}
		bash -e format_ucsc.sh
		"""
